{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e49525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from Dreamer import Dreamer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c70dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU selected\n"
     ]
    }
   ],
   "source": [
    "# Force CPU for debugging to avoid memory issues\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"GPU selected\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CPU selected for debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476c004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer_agent = Dreamer(\n",
    "    hidden_state_dims=100,\n",
    "    latent_state_dims=(32, 32),\n",
    "    observation_dims=(96, 96),\n",
    "    action_dims=3,\n",
    "    world_model_lr=2e-4,\n",
    "    world_model_betas=(0.9,0.999),\n",
    "    world_model_eps=1e-7,\n",
    "    WM_epochs=100,\n",
    "    beta_prediction=1.0,\n",
    "    beta_dynamics=0.1,\n",
    "    beta_representation=1.0,\n",
    "    critic_reward_buckets=255,\n",
    "    encoder_filter_num_1=32,\n",
    "    encoder_filter_num_2=64,\n",
    "    encoder_hidden_layer_nodes=400,\n",
    "    decoder_filter_num_1=64,\n",
    "    decoder_filter_num_2=32,\n",
    "    decoder_hidden_layer_nodes=100,\n",
    "    dyn_pred_hidden_num_nodes_1=100,\n",
    "    dyn_pred_hidden_num_nodes_2=100,\n",
    "    rew_pred_hidden_num_nodes_1=100,\n",
    "    rew_pred_hidden_num_nodes_2=100,\n",
    "    cont_pred_hidden_num_nodes_1=100,\n",
    "    cont_pred_hidden_num_nodes_2=100,\n",
    "    actor_lr=8e-5,\n",
    "    actor_betas=(0.9,0.999),\n",
    "    actor_eps=1e-7,\n",
    "    critic_lr=8e-5,\n",
    "    critic_betas=(0.9,0.999),\n",
    "    critic_eps=1e-7,\n",
    "    AC_epochs=10,\n",
    "    hidden_layer_actor_1_size=100,\n",
    "    hidden_layer_actor_2_size=100,\n",
    "    hidden_layer_critic_1_size=100,\n",
    "    hidden_layer_critic_2_size=100,\n",
    "    horizon=15,\n",
    "    batch_size=30,\n",
    "    training_iterations=10,\n",
    "    random_iterations=50,\n",
    "    nu=0.995,\n",
    "    lambda_=0.95,\n",
    "    gamma=0.99,\n",
    "    buffer_size=100000,\n",
    "    sequence_length=30,\n",
    "    seed=42,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b03204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - WorldModel params: 3319310\n",
      "  - Agent params: 271561\n",
      "  - Actor params: 123206\n",
      "  - Critic params: 148355\n",
      "  - Encoder params: 572816\n",
      "  - Decoder params: 2013114\n",
      "  - Sequence model params: 338700\n",
      "  - Dynamics predictor params: 123624\n",
      "  - Reward predictor params: 148355\n",
      "  - Continue predictor params: 122701\n"
     ]
    }
   ],
   "source": [
    "print(f\"  - WorldModel params: {sum(p.numel() for p in dreamer_agent.world_model.parameters())}\")\n",
    "print(f\"  - Agent params: {sum(p.numel() for p in dreamer_agent.agent.parameters())}\")\n",
    "print(f\"  - Actor params: {sum(p.numel() for p in dreamer_agent.agent.actor.parameters())}\")\n",
    "print(f\"  - Critic params: {sum(p.numel() for p in dreamer_agent.agent.critic.parameters())}\")\n",
    "print(f\"  - Encoder params: {sum(p.numel() for p in dreamer_agent.world_model.encoder.parameters())}\")\n",
    "print(f\"  - Decoder params: {sum(p.numel() for p in dreamer_agent.world_model.decoder.parameters())}\")\n",
    "print(f\"  - Sequence model params: {sum(p.numel() for p in dreamer_agent.world_model.sequence_model.parameters())}\")\n",
    "print(f\"  - Dynamics predictor params: {sum(p.numel() for p in dreamer_agent.world_model.dynamics_predictor.parameters())}\")\n",
    "print(f\"  - Reward predictor params: {sum(p.numel() for p in dreamer_agent.world_model.reward_predictor.parameters())}\")\n",
    "print(f\"  - Continue predictor params: {sum(p.numel() for p in dreamer_agent.world_model.continue_predictor.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CarRacing-v3\"\n",
    "env = gym.make(env_id, continuous=True)\n",
    "evaluation_env = gym.make(env_id, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Starting Random Kickstart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,96,96) into shape (96,96)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = \u001b[43mdreamer_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:286\u001b[39m, in \u001b[36mDreamer.train_dreamer\u001b[39m\u001b[34m(self, env, eval_env)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Random Kickstart.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.random_iterations), desc=\u001b[33m\"\u001b[39m\u001b[33mKickstarting Dreamer Agent.\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     WM_loss = \u001b[38;5;28mself\u001b[39m.train_world_model()\n\u001b[32m    288\u001b[39m     actor_loss, critic_loss = \u001b[38;5;28mself\u001b[39m.train_Agent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:186\u001b[39m, in \u001b[36mDreamer.rollout_policy\u001b[39m\u001b[34m(self, env, random_policy)\u001b[39m\n\u001b[32m    184\u001b[39m done = (terminated \u001b[38;5;129;01mor\u001b[39;00m truncated)\n\u001b[32m    185\u001b[39m continue_ = (\u001b[32m1\u001b[39m - done)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_to_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinue_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.seed += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Buffer.py:19\u001b[39m, in \u001b[36mBuffer.add_to_buffer\u001b[39m\u001b[34m(self, observation, action, reward, continue_)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_to_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation, action, reward, continue_):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext_idx\u001b[49m\u001b[43m]\u001b[49m = np.array(observation, dtype=np.uint8)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_buffer[\u001b[38;5;28mself\u001b[39m.next_idx] = np.array(action, dtype=np.float32)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.reward_buffer[\u001b[38;5;28mself\u001b[39m.next_idx] = np.array(reward, dtype=np.float32)\n",
      "\u001b[31mValueError\u001b[39m: could not broadcast input array from shape (3,96,96) into shape (96,96)"
     ]
    }
   ],
   "source": [
    "WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = dreamer_agent.train_dreamer(env, evaluation_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(WM_loss_list)\n",
    "plt.plot(actor_loss_list)\n",
    "plt.plot(critic_loss_list)\n",
    "plt.plot(evaluation_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_RL_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

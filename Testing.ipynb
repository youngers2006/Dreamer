{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e49525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from Dreamer import Dreamer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c70dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU selected\n"
     ]
    }
   ],
   "source": [
    "# Force CPU for debugging to avoid memory issues\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"GPU selected\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CPU selected for debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476c004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer_agent = Dreamer(\n",
    "    hidden_state_dims=100,\n",
    "    latent_state_dims=(32, 32),\n",
    "    observation_dims=(96, 96),\n",
    "    action_dims=3,\n",
    "    world_model_lr=2e-4,\n",
    "    world_model_betas=(0.9,0.999),\n",
    "    world_model_eps=1e-7,\n",
    "    WM_epochs=100,\n",
    "    beta_prediction=1.0,\n",
    "    beta_dynamics=0.1,\n",
    "    beta_representation=1.0,\n",
    "    critic_reward_buckets=255,\n",
    "    encoder_filter_num_1=32,\n",
    "    encoder_filter_num_2=64,\n",
    "    encoder_hidden_layer_nodes=400,\n",
    "    decoder_filter_num_1=64,\n",
    "    decoder_filter_num_2=32,\n",
    "    decoder_hidden_layer_nodes=100,\n",
    "    dyn_pred_hidden_num_nodes_1=100,\n",
    "    dyn_pred_hidden_num_nodes_2=100,\n",
    "    rew_pred_hidden_num_nodes_1=100,\n",
    "    rew_pred_hidden_num_nodes_2=100,\n",
    "    cont_pred_hidden_num_nodes_1=100,\n",
    "    cont_pred_hidden_num_nodes_2=100,\n",
    "    actor_lr=8e-5,\n",
    "    actor_betas=(0.9,0.999),\n",
    "    actor_eps=1e-7,\n",
    "    critic_lr=8e-5,\n",
    "    critic_betas=(0.9,0.999),\n",
    "    critic_eps=1e-7,\n",
    "    AC_epochs=10,\n",
    "    hidden_layer_actor_1_size=100,\n",
    "    hidden_layer_actor_2_size=100,\n",
    "    hidden_layer_critic_1_size=100,\n",
    "    hidden_layer_critic_2_size=100,\n",
    "    horizon=15,\n",
    "    batch_size=30,\n",
    "    training_iterations=10,\n",
    "    random_iterations=50,\n",
    "    nu=0.995,\n",
    "    lambda_=0.95,\n",
    "    gamma=0.99,\n",
    "    buffer_size=100000,\n",
    "    sequence_length=30,\n",
    "    seed=42,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b03204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - WorldModel params: 3319310\n",
      "  - Agent params: 271561\n",
      "  - Actor params: 123206\n",
      "  - Critic params: 148355\n",
      "  - Encoder params: 572816\n",
      "  - Decoder params: 2013114\n",
      "  - Sequence model params: 338700\n",
      "  - Dynamics predictor params: 123624\n",
      "  - Reward predictor params: 148355\n",
      "  - Continue predictor params: 122701\n"
     ]
    }
   ],
   "source": [
    "print(f\"  - WorldModel params: {sum(p.numel() for p in dreamer_agent.world_model.parameters())}\")\n",
    "print(f\"  - Agent params: {sum(p.numel() for p in dreamer_agent.agent.parameters())}\")\n",
    "print(f\"  - Actor params: {sum(p.numel() for p in dreamer_agent.agent.actor.parameters())}\")\n",
    "print(f\"  - Critic params: {sum(p.numel() for p in dreamer_agent.agent.critic.parameters())}\")\n",
    "print(f\"  - Encoder params: {sum(p.numel() for p in dreamer_agent.world_model.encoder.parameters())}\")\n",
    "print(f\"  - Decoder params: {sum(p.numel() for p in dreamer_agent.world_model.decoder.parameters())}\")\n",
    "print(f\"  - Sequence model params: {sum(p.numel() for p in dreamer_agent.world_model.sequence_model.parameters())}\")\n",
    "print(f\"  - Dynamics predictor params: {sum(p.numel() for p in dreamer_agent.world_model.dynamics_predictor.parameters())}\")\n",
    "print(f\"  - Reward predictor params: {sum(p.numel() for p in dreamer_agent.world_model.reward_predictor.parameters())}\")\n",
    "print(f\"  - Continue predictor params: {sum(p.numel() for p in dreamer_agent.world_model.continue_predictor.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CarRacing-v3\"\n",
    "env = gym.make(env_id, continuous=True)\n",
    "evaluation_env = gym.make(env_id, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Starting Random Kickstart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kickstarting Dreamer Agent.:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = \u001b[43mdreamer_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:286\u001b[39m, in \u001b[36mDreamer.train_dreamer\u001b[39m\u001b[34m(self, env, eval_env)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Random Kickstart.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.random_iterations), desc=\u001b[33m\"\u001b[39m\u001b[33mKickstarting Dreamer Agent.\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     WM_loss = \u001b[38;5;28mself\u001b[39m.train_world_model()\n\u001b[32m    288\u001b[39m     actor_loss, critic_loss = \u001b[38;5;28mself\u001b[39m.train_Agent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:196\u001b[39m, in \u001b[36mDreamer.rollout_policy\u001b[39m\u001b[34m(self, env, random_policy)\u001b[39m\n\u001b[32m    194\u001b[39m     latent_state, _ = \u001b[38;5;28mself\u001b[39m.world_model.encoder.encode(hidden_state, observation_tensor)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     hidden_state, latent_state, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation__tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     observation = observation_\n\u001b[32m    198\u001b[39m     observation_tensor = observation__tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/WorldModel.py:77\u001b[39m, in \u001b[36mWorldModel.observe_step\u001b[39m\u001b[34m(self, last_latent, last_hidden, last_action, observation)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, last_latent, last_hidden, last_action, observation) -> \u001b[38;5;28mtuple\u001b[39m[torch.tensor, torch.tensor, torch.tensor]:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     hidden_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msequence_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_latent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     latent_state, latent_logits = \u001b[38;5;28mself\u001b[39m.encoder.encode(hidden_state, observation)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m latent_state, hidden_state, latent_logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/SequenceModel.py:23\u001b[39m, in \u001b[36mSequenceModel.forward\u001b[39m\u001b[34m(self, last_latent_state, last_hidden_state, last_action)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, last_latent_state: torch.tensor, last_hidden_state: torch.tensor, last_action: torch.tensor):\n\u001b[32m     22\u001b[39m     last_latent_state = \u001b[38;5;28mself\u001b[39m.flatten(last_latent_state)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_latent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_action\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     25\u001b[39m     last_hidden_state = last_hidden_state.unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: expected Tensor as element 1 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = dreamer_agent.train_dreamer(env, evaluation_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(WM_loss_list)\n",
    "plt.plot(actor_loss_list)\n",
    "plt.plot(critic_loss_list)\n",
    "plt.plot(evaluation_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_RL_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e49525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from Dreamer import Dreamer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c70dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU selected\n"
     ]
    }
   ],
   "source": [
    "# Force CPU for debugging to avoid memory issues\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"GPU selected\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CPU selected for debugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476c004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dreamer_agent = Dreamer(\n",
    "    hidden_state_dims=100,\n",
    "    latent_state_dims=(32, 32),\n",
    "    observation_dims=(96, 96),\n",
    "    action_dims=3,\n",
    "    world_model_lr=2e-4,\n",
    "    world_model_betas=(0.9,0.999),\n",
    "    world_model_eps=1e-7,\n",
    "    WM_epochs=100,\n",
    "    beta_prediction=1.0,\n",
    "    beta_dynamics=0.1,\n",
    "    beta_representation=1.0,\n",
    "    critic_reward_buckets=255,\n",
    "    encoder_filter_num_1=32,\n",
    "    encoder_filter_num_2=64,\n",
    "    encoder_hidden_layer_nodes=400,\n",
    "    decoder_filter_num_1=64,\n",
    "    decoder_filter_num_2=32,\n",
    "    decoder_hidden_layer_nodes=100,\n",
    "    dyn_pred_hidden_num_nodes_1=100,\n",
    "    dyn_pred_hidden_num_nodes_2=100,\n",
    "    rew_pred_hidden_num_nodes_1=100,\n",
    "    rew_pred_hidden_num_nodes_2=100,\n",
    "    cont_pred_hidden_num_nodes_1=100,\n",
    "    cont_pred_hidden_num_nodes_2=100,\n",
    "    actor_lr=8e-5,\n",
    "    actor_betas=(0.9,0.999),\n",
    "    actor_eps=1e-7,\n",
    "    critic_lr=8e-5,\n",
    "    critic_betas=(0.9,0.999),\n",
    "    critic_eps=1e-7,\n",
    "    AC_epochs=10,\n",
    "    hidden_layer_actor_1_size=100,\n",
    "    hidden_layer_actor_2_size=100,\n",
    "    hidden_layer_critic_1_size=100,\n",
    "    hidden_layer_critic_2_size=100,\n",
    "    horizon=15,\n",
    "    batch_size=30,\n",
    "    training_iterations=10,\n",
    "    random_iterations=50,\n",
    "    nu=0.995,\n",
    "    lambda_=0.95,\n",
    "    gamma=0.99,\n",
    "    buffer_size=100000,\n",
    "    sequence_length=30,\n",
    "    seed=42,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b03204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - WorldModel params: 3319310\n",
      "  - Agent params: 271561\n",
      "  - Actor params: 123206\n",
      "  - Critic params: 148355\n",
      "  - Encoder params: 572816\n",
      "  - Decoder params: 2013114\n",
      "  - Sequence model params: 338700\n",
      "  - Dynamics predictor params: 123624\n",
      "  - Reward predictor params: 148355\n",
      "  - Continue predictor params: 122701\n"
     ]
    }
   ],
   "source": [
    "print(f\"  - WorldModel params: {sum(p.numel() for p in dreamer_agent.world_model.parameters())}\")\n",
    "print(f\"  - Agent params: {sum(p.numel() for p in dreamer_agent.agent.parameters())}\")\n",
    "print(f\"  - Actor params: {sum(p.numel() for p in dreamer_agent.agent.actor.parameters())}\")\n",
    "print(f\"  - Critic params: {sum(p.numel() for p in dreamer_agent.agent.critic.parameters())}\")\n",
    "print(f\"  - Encoder params: {sum(p.numel() for p in dreamer_agent.world_model.encoder.parameters())}\")\n",
    "print(f\"  - Decoder params: {sum(p.numel() for p in dreamer_agent.world_model.decoder.parameters())}\")\n",
    "print(f\"  - Sequence model params: {sum(p.numel() for p in dreamer_agent.world_model.sequence_model.parameters())}\")\n",
    "print(f\"  - Dynamics predictor params: {sum(p.numel() for p in dreamer_agent.world_model.dynamics_predictor.parameters())}\")\n",
    "print(f\"  - Reward predictor params: {sum(p.numel() for p in dreamer_agent.world_model.reward_predictor.parameters())}\")\n",
    "print(f\"  - Continue predictor params: {sum(p.numel() for p in dreamer_agent.world_model.continue_predictor.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d6f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CarRacing-v3\"\n",
    "env = gym.make(env_id, continuous=True)\n",
    "evaluation_env = gym.make(env_id, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Starting Random Kickstart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kickstarting Dreamer Agent.:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = \u001b[43mdreamer_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:288\u001b[39m, in \u001b[36mDreamer.train_dreamer\u001b[39m\u001b[34m(self, env, eval_env)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.random_iterations), desc=\u001b[33m\"\u001b[39m\u001b[33mKickstarting Dreamer Agent.\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m.rollout_policy(env, random_policy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     WM_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_world_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     actor_loss, critic_loss = \u001b[38;5;28mself\u001b[39m.train_Agent()\n\u001b[32m    290\u001b[39m     WM_loss_list.append(WM_loss) ; actor_loss_list.append(actor_loss) ; critic_loss_list.append(critic_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/Dreamer.py:207\u001b[39m, in \u001b[36mDreamer.train_world_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.WM_epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining World Model On Buffer Data\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    204\u001b[39m     observation_seq_batch, action_seq_batch, reward_seq_batch, continue_seq_batch, _ = \u001b[38;5;28mself\u001b[39m.buffer.sample_sequences(\n\u001b[32m    205\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m    206\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     loss_world_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_seq_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_seq_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_seq_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinue_seq_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     loss_list.append(loss_world_model)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/WorldModel.py:148\u001b[39m, in \u001b[36mWorldModel.training_step\u001b[39m\u001b[34m(self, observation_sequences, action_sequences, reward_sequences, continue_sequences)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\n\u001b[32m    142\u001b[39m         \u001b[38;5;28mself\u001b[39m, \n\u001b[32m    143\u001b[39m         observation_sequences: torch.tensor, \n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m         continue_sequences: torch.tensor\n\u001b[32m    147\u001b[39m     ): \u001b[38;5;66;03m# (batch_size, sequence_length, features)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     prior_logits, posterior_logits, obs_log_lh, rew_log_lh, cont_log_lh = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munroll_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontinue_sequences\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     prior_dist = torch.distributions.Categorical(logits=prior_logits)\n\u001b[32m    155\u001b[39m     posterior_dist = torch.distributions.Categorical(logits=posterior_logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/WorldModel.py:133\u001b[39m, in \u001b[36mWorldModel.unroll_model\u001b[39m\u001b[34m(self, observation_sequence_batch, action_sequence_batch, reward_sequence_batch, continue_sequence_batch)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prior_logits, posterior_logits, obs_likelyhood_seq, rew_likelyhood_seq, cont_likelyhood_seq\n\u001b[32m    132\u001b[39m batched_sequence_unroll = torch.vmap(single_sequence_unroll, in_dims=(\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m prior_logits, posterior_logits, obs_log_lh, rew_log_lh, cont_log_lh = \u001b[43mbatched_sequence_unroll\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservation_sequence_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_sequence_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_sequence_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinue_sequence_batch\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prior_logits, posterior_logits, obs_log_lh, rew_log_lh, cont_log_lh\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_RL_gym/lib/python3.13/site-packages/torch/_functorch/apis.py:202\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_RL_gym/lib/python3.13/site-packages/torch/_functorch/vmap.py:334\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    324\u001b[39m         func,\n\u001b[32m    325\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         **kwargs,\n\u001b[32m    331\u001b[39m     )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pytorch_RL_gym/lib/python3.13/site-packages/torch/_functorch/vmap.py:484\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    481\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    482\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/WorldModel.py:96\u001b[39m, in \u001b[36mWorldModel.unroll_model.<locals>.single_sequence_unroll\u001b[39m\u001b[34m(observation_sequence, action_sequence, reward_sequence, continue_sequence)\u001b[39m\n\u001b[32m     93\u001b[39m cont_likelyhood_seq = []\n\u001b[32m     95\u001b[39m hidden_state = torch.zeros(\u001b[38;5;28mself\u001b[39m.hidden_dims, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m posterior_latent, posterior_logits_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.horizon):\n\u001b[32m     99\u001b[39m     prev_action = action_sequence[t-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m t > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.zeros(\u001b[38;5;28mself\u001b[39m.action_dims, device=\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/VariationalAutoEncoder.py:40\u001b[39m, in \u001b[36mEncoder.encode\u001b[39m\u001b[34m(self, hidden_state, observation)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state, observation):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     dist = torch.distributions.Categorical(logits=logits)\n\u001b[32m     42\u001b[39m     sampled_idx = dist.sample()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Dreamer/VariationalAutoEncoder.py:35\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, hidden, latent)\u001b[39m\n\u001b[32m     33\u001b[39m features = \u001b[38;5;28mself\u001b[39m.feature_extractor(latent)\n\u001b[32m     34\u001b[39m features = \u001b[38;5;28mself\u001b[39m.flatten(features)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.latent_mapper(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensors must have same number of dimensions: got 3 and 1"
     ]
    }
   ],
   "source": [
    "WM_loss_list, actor_loss_list, critic_loss_list, evaluation_list = dreamer_agent.train_dreamer(env, evaluation_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(WM_loss_list)\n",
    "plt.plot(actor_loss_list)\n",
    "plt.plot(critic_loss_list)\n",
    "plt.plot(evaluation_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_RL_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
